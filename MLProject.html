<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <style>
      html,
      body {
        margin: 0;
        height: 100vh;
        width: 100vw;
      }
      * {
        box-sizing: border-box;
      }
      div {
        max-width: 900px;
        min-width: 400px;
        margin: auto;
        text-align: justify;
        text-justify: inter-word;
        padding: 25px 20px 30px 20px;
        font-size: 16px;
        line-height: 21px;

        /*background-color: lightskyblue;*/
      }

      @import url("https://rsms.me/inter/inter.css");
      html {
        font-family: "Inter", sans-serif;
      }
      @supports (font-variation-settings: normal) {
        html {
          font-family: "Inter var", sans-serif;
        }
      }
      body {
        font-family: Helvetica;
        font-size: 13px;
      }

      img {
        border-radius: 2px;
        margin: auto;
        display: block;
        border: 1px solid white;
      }
      .wbox {
        border-radius: 2px 2px 2px;
        font-size: 16px;
        background: #99ff99;
        color: #212121;
        padding: 8px;
        border: 1px solid white;
        box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.2),
          0 2px 5px 0 rgba(0, 0, 0, 0.19);
        margin: auto;
        min-width: 35%;
        max-width: 65%;
      }
      table {
        border-collapse: collapse;
        width: 90%;
        border: 2px solid black;
        margin: auto;
      }

      th,
      td {
        padding: 10px;
        text-align: left;
        border-bottom: 1px solid #ddd;
      }
      th {
        background-color: #2F4F4F;
        color: white;
        border: 2px solid black;
      }
      tr:nth-child(even) {
        background-color: #f2f2f2;
      }
      tr:hover {
        background-color: #D3D3D3;
      }
      td {
        border: 1px solid black;
      }
      a:link {
        color: #4169E1;
        background-color: transparent;
        text-decoration: none;
      }

      a:visited {
        color: #4D71A3;
        background-color: transparent;
        text-decoration: none;
      }

      a:hover {
        color: red;
        background-color: transparent;
        text-decoration: underline;
      }

      a:active {
        color: yellow;
        background-color: transparent;
        text-decoration: underline;
      }
    </style>
    <title>Group-7 Project</title>

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>
  </head>

  <body>
    <div class="container">
      <h1 style="text-align:center; padding: 10px;line-height: 34px;">
        Modification in the stack first approach to detect WHIM using Planck
        data
      </h1>
      <p>Project by: Akshay Priyadarshi and Pratyush Kumar Das</p>
      <hr />
      <h2>Motivation</h2>
      <p>
        The project aims to detect the diffused thermal
        <a
          href="https://ned.ipac.caltech.edu/level5/Birkinshaw/Birk4_1.html"
          target="_blank"
          >Sunyaev-Zeldovich</a
        >
        (tSZ) effect from the
        <a
          href="https://astronomy.swin.edu.au/cosmos/G/Galactic+Filaments"
          target="_blank"
          >Gas Filaments</a
        >

        between the
        <a
          href="http://classic.sdss.org/dr3/products/general/edr_html/node53.html"
          target="_blank"
          >Luminous Red Galaxy</a
        >
        (LRG) pairs by stacking the individual frequency maps. The detection of
        SZ can help in determining and verifying the value of the Hubble
        constant. Basically in the ILC algorithm, we compare the temperatures of
        the stacked image generated by us with the source image and try to
        minimize the temperature difference. We hope that instead of assigning a
        single weight to each sub-image, if we assign pixel dependent weights
        then we can reach closer to the source image, thereby reducing the
        error. This method of computation will also be tested if it helps in
        reducing the runtime compared to the previous method.
      </p>
      <h2>Data Source:</h2>
      <p>
        We will use around 100,000 LRG pairs from the
        <a href="https://www.sdss.org/dr12/" target="_blank"
          >SDSS DR12 catalogue</a
        >
        . The selection criterion ensures that the data will have minimal
        contamination, i.e securing data from the
        <a
          href="https://www.cosmos.esa.int/documents/387566/428323/47ESLAB_April_04_10_20_Combet.pdf/50beb740-e50b-4f27-93d1-2e87ce5a1d19"
          target="_blank"
          >Galactic CO emission</a
        >
        as well as the tSZ signal from the
        <a
          href="https://imagine.gsfc.nasa.gov/science/objects/clusters.html"
          target="_blank"
          >clusters of galaxies</a
        >.
      </p>
      <h2>Stack First approach:</h2>
      <p>
        In this process the authors of the source paper have first stacked the
        Planck channel maps and then performed the Internal Linear Combination
        method to extract the diffused y<sub>sz</sub> signal. This approach
        makes the component separation a lot easier as the stacking greatly
        suppresses the noise and the
        <a href="https://webhome.phy.duke.edu/~kolena/cmb.html" target="_blank"
          >Cosmological Microwave Background (CMB)</a
        >
        contributions. As the dust foreground turns homogeneous throughout the
        stacked patch in the spectral domain, it helps in component separation.
        In this way the CMB component is removed and the remaining foreground
        turns simpler even before we use the component separation algorithm.
      </p>
      <figure>
        <img
          src="https://i.imgur.com/7LerOKQ.jpg"
          alt="Example of stacking images"
          style="width:70%;"
        />
        <p>
          <figcaption
            style="text-align:center;font-size:16px;font-style: italic;"
          >
            Fig. Example of stacking images (credits:
            <a
              href="https://chandra.harvard.edu/photo/2002/0157/more.html"
              target="_blank"
              >Chandra, Harvard</a
            >)
          </figcaption>
        </p>
      </figure>

      <h2>Mechanism:</h2>
      <p>
        Maps of LRG pairs were taken in 6 different frequency ranges from 70 GHz
        to 545 GHz. There might be several sources of emission in the given
        frequency range, so the total emission observed at a given frequency
        channel map, \(x_i\) , can be expressed using a linear equation:
        \[x_{i}=\Sigma_{j=1}^{N}a_{ij}(p)S_j(p) + n_i(p)\] This equation gives
        the superposition of \(N\) different astrophysical components (\(S_j\))
        and instrumental noise \(n_i\) where we know the emission law of
        individual sources. Here, p denotes the pixel number, and \(a_{ij}\) is
        the mixing matrix which gives the weight of each individual sources in
        the map. The previous equation can be simplified further by taking the
        assumption that the weight matrix is uniform throughout the sky.
        \[x(p)=aS(p) + n(p)\] In this equation x(p) will be the vector of N
        observations, S(p) will be a matrix related to all the N source maps, n
        will be the vector for all the N noise maps. Using the Internal Linear
        Combination (ILC) Algorithm, the astrophysical components can be
        calculated from the observed map as \[S(p)=\Sigma_iw_ix_i(p)\] where,
        \(w_i\) are the weights. [To try out this approximation of getting
        weights, we plotted it graphically for numbers instead of maps, and it
        seemed to hold for a wide range of values]. The weights of this linear
        combination were calculated by minimising the variance of "S", and
        taking into consideration that "S" is preserved, i.e. \(\Sigma
        w_ia_i=1\)
      </p>
      <p>
        Similarly, a frequency map might have CMB, dust, foreground, noise, and
        tSZ components. The tSZ component that we are interested in can be
        extracted using this method of ILC, i.e.
        \(y_{sz}(p)=\Sigma_iw_ix_i(p)\), where \(x_i\) are the maps at 6
        different frequency ranges.
      </p>
      <figure>
        <img
          src="https://i.imgur.com/UM6om4i.png"
          alt="Explnation for Internal Linear Combination"
          style="width:70%;"
        />
        <p>
          <figcaption
            style="text-align:center;font-size:16px;font-style: italic;"
          >
            Fig. Block diagram of getting tSZ component from 6 frequency maps.
          </figcaption>
        </p>
      </figure>

      <hr />

      <h2>Our proposed plan and modification</h2>
      <p>
        The basic change that we propose is, while calculating weights we plan
        to consider individul pixels instead of the whole image. Earlier the
        weights generated were of the form of uniform matrix, but now each
        component of the matrix will be dependent on individual pixels of each
        sub-image.
      </p>
      <iframe
        width="755"
        height="453"
        src="https://www.youtube.com/embed/VMKS3upNFU4"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
        allowfullscreen
      ></iframe>
      <hr />
      <h2>Timeline:</h2>
      <p>
        The project timeline:
      </p>
      <table>
        <tr>
          <th>Week</th>
          <th>Initially Proposed Work</th>
          <th>Updates/Progress on the go</th>
        </tr>
        <tr>
          <td><b>Week 4:</b></td>
          <td>
            Proposal and Initiation. Searching for appropriate reading material
            and sources.
          </td>
          <td>Done as planned.</td>
        </tr>
        <tr>
          <td><b>Week 5:</b></td>
          <td>
            Literature-Reading and understanding the science and computation
            behind the project.
          </td>
          <td>
            Initiated the plan. We have many technical doubts which are being
            resolved on the way.
          </td>
        </tr>
        <tr>
          <td><b>Week 6:</b></td>
          <td><i>continued from the previous week.</i></td>
          <td>
            Finding difficulty in understanding the concept. So we tried to
            understand it computationally while applying it. Started the work of
            week 7 and 8.
          </td>
        </tr>
        <tr>
          <td><b>Week 7:</b></td>
          <td>Data Selection for the project. Milestone Presentation.</td>
          <td>
            Milestone presentation postponed. Started the code implementation
            and testing on a small scale.
          </td>
        </tr>
        <tr>
          <td><b>Week 8:</b></td>
          <td>Code preparation and Training.</td>
          <td>Milestone presentation to be done along with a test run.</td>
        </tr>
        <tr>
          <td><b>Week 9:</b></td>
          <td>Testing the code.</td>
          <td>
            A ML code on linear regression was prepared. The results were
            suitable and up to the mark. This led us to complete the planned
            code implementation.
          </td>
        </tr>
        <tr>
          <td><b>Week 10:</b></td>
          <td>Code implementation.</td>
          <td>
            As the proposed task was over, we tried implementing different
            methods viz. ANN and SVR.
          </td>
        </tr>
        <tr>
          <td><b>Week 11:</b></td>
          <td><i>continued from the previous week.</i></td>
          <td>
            Faced some difficulty in implementing ANN due to computational
            limitations. But, both ANN and SVR (linear and rbf) implementation
            was done and result analysis is awaited.
          </td>
        </tr>
        <tr>
          <td><b>Week 12:</b></td>
          <td>
            Additional work of implementing Random Forest algorithm was done.
            The 5 different models were compared. Result analysis, error,
            conclusion. Final Report Presentation.
          </td>
          <td>As per the proposal.</td>
        </tr>
      </table>

      <p>
        The changes in the timeline are updated through the course of the
        project in the updates column.
      </p>
      <hr />
      <h2>Tentative Algortihm and implementation</h2>
      <p>
        <b>Step-1:</b> We will select a random number of pairs from the 88,000
        samples that we have, and then stack on the simulated frequency maps,
        and the LIL map. Using this method, we hope to generate 100- 1000 random
        samples which will be our training dataset.
      </p>
      <p>
        <b>Step-2:</b> Next, we will train the Neural network using this
        training dataset and get the values of pixel dependent weight maps such
        that the difference between the linearly combined map and the stacked
        LIL maps is minimum.
      </p>
      <p>
        <b>Step-3:</b> Subsequently, we will use some other stacked pairs, which
        are different from the training set, to Test the accuracy of our trained
        model.
      </p>
      <p>
        <b>Step-4:</b> If the predictions is satisfactory, we will implement the
        code on all the remaining data available. Then, we'll compare the
        outputs with the respective reported results.
      </p>
      <p>
        <b>Step-5 (New):</b> 5 different algorithms were used to generate the
        model. The results were compared and the best was taken for
        implementation on rest of the dataset.
      </p>

      <hr />
      <h3>Ensuing Tasks:</h3>
      We have divided our project in 3 sections.
      <p><b>First chapter:</b> Understanding the science behind our project</p>
      <p>
        <b>Second chapter:</b> In this section we will choose our training set
        appropriately and train the neural network. Finally test the accuracy
        using the test data.
      </p>
      <p>
        <b>Final chapter:</b> In this section we will we will implement the
        upgraded algorithm. Since we have a huge dataset we will also use MPI
        and parallelize the code.
      </p>

      <hr />

      <h2>
        Training and Testing
      </h2>

      <figure>
        <img
          src="https://i.imgur.com/fDXQkNl.png"
          alt="X used for testing"
          style="width:100%;"
        />
        <p>
          <figcaption
            style="text-align:center;font-size:16px;font-style: italic;"
          >
            Fig. A sample of 6 "x" values used for testing. Corresponding "y"
            for these values is the Y\(_{test}\).
          </figcaption>
        </p>
      </figure>
      <p>
        As an initial run, a ML codes were prepared and run on randomly selected
        100 samples. 99 samples were used for training and the rest 1 sample
        were used for testing.
      </p>

      <h3>
        1. Linear Regression:
      </h3>
      <p>
        "<a
          href="https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html"
          target="_blank"
          >Linear Regression</a
        >
        is a supervised machine learning algorithm where the predicted output is
        continuous and has a constant slope."
      </p>

      <figure>
        <img
          src="https://i.imgur.com/i8LxXpq.png"
          alt="Code Snippet"
          style="width:90%;"
        />
        <p>
          <figcaption
            style="text-align:center;font-size:16px;font-style: italic;"
          >
            Fig. Code Snippet.
          </figcaption>
        </p>
      </figure>

      <figure>
        <img
          src="https://i.imgur.com/aaApQvr.png"
          alt="Weights generated from the model"
          style="width:100%;"
        />
        <p>
          <figcaption
            style="text-align:center;font-size:16px;font-style: italic;"
          >
            Fig. Example of Weights generated from the linear regression model.
          </figcaption>
        </p>
      </figure>

      <figure>
        <img
          src="https://i.imgur.com/vJF9p3F.png"
          alt="Y_test and Y_found"
          style="width:80%;"
        />
        <p>
          <figcaption
            style="text-align:center;font-size:16px;font-style: italic;"
          >
            Fig. Example of Y\(_{test}\) (left) and Y\(_{found}\)(right).
          </figcaption>
        </p>
      </figure>
      <figure>
        <img
          src="https://i.imgur.com/maZaWNq.png"
          alt="Residual Maps"
          style="width:80%;"
        />
        <p>
          <figcaption
            style="text-align:center;font-size:16px;font-style: italic;"
          >
            Fig. Example of Residual Maps.
          </figcaption>
        </p>
      </figure>

      <p>
        The accuracy of the found values using this method came out to be very
        high.
      </p>
      <h3>
        2. ANN:
      </h3>
      <p>
        "<a
          href="https://towardsdatascience.com/applied-deep-learning-part-1-artificial-neural-networks-d7834f67a4f6"
          target="_blank"
          >Artificial Neural Networks</a
        >
        (ANN) are multi-layer fully-connected neural nets. They consist of an
        input layer, multiple hidden layers, and an output layer."
      </p>

      <figure>
        <img
          src="https://i.imgur.com/4r5nxoV.png"
          alt="Code Snippet"
          style="width:90%;"
        />
        <p>
          <figcaption
            style="text-align:center;font-size:16px;font-style: italic;"
          >
            Fig. Code Snippet.
          </figcaption>
        </p>
      </figure>

      We have only 99 set of images and the model couldnt train properly using
      such less dataset. The prediction was really bad, so we dropped the idea
      of using ANN.

      <h3>
        3. SVR:
      </h3>
      <p>
        "The objective of the
        <a
          href="https://towardsdatascience.com/an-introduction-to-support-vector-regression-svr-a3ebc1672c2"
          target="_blank"
          >Support Vector Machine - Regression</a
        >
        (SVR) gives us the flexibility to define how much error is acceptable in
        our model and will find an appropriate line (or hyperplane in higher
        dimensions) to fit the data."
      </p>
      <figure>
        <img
          src="https://i.imgur.com/aS9LrpT.png"
          alt="Code Snippet"
          style="width:90%;"
        />
        <p>
          <figcaption
            style="text-align:center;font-size:16px;font-style: italic;"
          >
            Fig. Code Snippet.
          </figcaption>
        </p>
      </figure>
      <h4>
        a) SVR 'rbf'
      </h4>
      <p>
        "Gaussian RBF (Radial Basis Function) is one of the popular Kernel
        methods used in SVR models for more. RBF kernel is a function whose
        value depends on the distance from the origin or from some point." (<a
          href="https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47"
          target="_blank"
          >support vector machine</a
        >)
      </p>

      <figure>
        <img
          src="https://i.imgur.com/tsmmCTx.png"
          alt="Y_test and Y_found"
          style="width:80%;"
        />
        <p>
          <figcaption
            style="text-align:center;font-size:16px;font-style: italic;"
          >
            Fig. Y\(_{test}\) (left) and Y\(_{found}\)(right).
          </figcaption>
        </p>
      </figure>
      <figure>
        <img
          src="https://i.imgur.com/elDAI1c.png"
          alt="Residual Maps"
          style="width:80%;"
        />
        <p>
          <figcaption
            style="text-align:center;font-size:16px;font-style: italic;"
          >
            Fig. Residual Maps. Residual (left) and Percentage Residual (right)
          </figcaption>
        </p>
      </figure>
      <p>
        The accuracy of the found values using this method came out to be very
        low.
      </p>
      <h4>
        b) SVR 'linear'
      </h4>

      <figure>
        <img
          src="https://i.imgur.com/JXWGSxL.png"
          alt="Y_test and Y_found"
          style="width:80%;"
        />
        <p>
          <figcaption
            style="text-align:center;font-size:16px;font-style: italic;"
          >
            Fig. Y\(_{test}\) (left) and Y\(_{found}\)(right).
          </figcaption>
        </p>
      </figure>
      <figure>
        <img
          src="https://i.imgur.com/BC3mW4G.png"
          alt="Residual Maps"
          style="width:80%;"
        />
        <p>
          <figcaption
            style="text-align:center;font-size:16px;font-style: italic;"
          >
            Fig. Residual Maps. Residual (left) and Percentage Residual (right)
          </figcaption>
        </p>
      </figure>
      <p>
        The accuracy of the found values using this method was very low.
      </p>
      As none of the results from SVR are good enough, we dropped the idea of
      using SVR too, but the results gave a hint that linear model ran better
      than rbf so we can expect our dataset will work better for other linear
      models as well.
      <h3>
        4. Random Forest:
      </h3>
      <p>
        "<a
          href="https://towardsdatascience.com/understanding-random-forest-58381e0602d2"
          target="_blank"
          >Random forest</a
        >, like its name implies, consists of a large number of individual
        decision trees that operate as an ensemble. Each individual tree in the
        random forest spits out a class prediction and the class with the most
        votes becomes our model’s prediction."
      </p>
      <figure>
        <img
          src="https://i.imgur.com/KUnPxJH.png"
          alt="Y_test and Y_found"
          style="width:80%;"
        />
        <p>
          <figcaption
            style="text-align:center;font-size:16px;font-style: italic;"
          >
            Fig. Y\(_{test}\) (left) and Y\(_{found}\)(right).
          </figcaption>
        </p>
      </figure>
      <figure>
        <img
          src="https://i.imgur.com/6W57LYw.png"
          alt="Residual Maps"
          style="width:80%;"
        />
        <p>
          <figcaption
            style="text-align:center;font-size:16px;font-style: italic;"
          >
            Fig. Residual Maps. Residual (left) and Percentage Residual (right)
          </figcaption>
        </p>
      </figure>

      <hr />
      <h3>
        Summary of output
      </h3>
      <table>
        <tr>
          <th>S.N.</th>
          <th>Algorithm</th>
          <th>R\(^2\) (Higher is better)</th>
          <th>MSE (Lower is better)</th>
        </tr>
        <tr>
          <td><b>0.</b></td>
          <td>
            Stack First Approach (Source Paper)
          </td>
          <td>0.59</td>
          <td>1.75xE-16</td>
        </tr>
        <tr>
          <td><b>1.</b></td>
          <td>
            Linear Regression
          </td>
          <td>0.66</td>
          <td>8.10xE-17</td>
        </tr>
        <tr>
          <td><b>2.</b></td>
          <td>
            ANN
          </td>
          <td>NA</td>
          <td>NA</td>
        </tr>
        <tr>
          <td><b>3.</b></td>
          <td>
            SVR 'rbf'
          </td>
          <td>-2.86</td>
          <td>1.38xE-15</td>
        </tr>
        <tr>
          <td><b>4.</b></td>
          <td>
            SVR 'linear'
          </td>
          <td>0.01</td>
          <td>4.86xE-16</td>
        </tr>
        <tr>
          <td><b>5.</b></td>
          <td>
            Random Forest
          </td>
          <td>0.63</td>
          <td>1.0xE-16</td>
        </tr>
      </table>

      <hr />
      <h2>
        Comparison with ILC
      </h2>
      <h3>
        1. ILC vs Linear Regression
      </h3>
      Comparing the results from Linear Regression algorithm with the
      traditional ILC method.
      <figure>
        <img
          src="https://i.imgur.com/HLQ4hUA.png"
          alt="Y\(_{ILC}\) (left) and Y\(_{Linear Regression}\) (right)"
          style="width:80%;"
        />
        <p>
          <figcaption
            style="text-align:center;font-size:16px;font-style: italic;"
          >
            Fig. Y\(_{ILC}\) (left) and Y\(_{Linear Regression}\) (right).
          </figcaption>
        </p>
      </figure>
      <figure>
        <img
          src="https://i.imgur.com/upCQzYA.png"
          alt="Residual Maps"
          style="width:80%;"
        />
        <p>
          <figcaption
            style="text-align:center;font-size:16px;font-style: italic;"
          >
            Fig. Residual Maps. Residual (left) and Percentage Residual (right)
          </figcaption>
        </p>
      </figure>

      <h3>
        2. ILC vs Random Forest
      </h3>
      <figure>
        <img
          src="https://i.imgur.com/8TMqx5g.png"
          alt="Y\(_{ILC}\) (left) and Y\(_{Linear Regression}\) (right)"
          style="width:80%;"
        />
        <p>
          <figcaption
            style="text-align:center;font-size:16px;font-style: italic;"
          >
            Fig. Y\(_{ILC}\) (left) and Y\(_{Linear Regression}\) (right).
          </figcaption>
        </p>
      </figure>
      <figure>
        <img
          src="https://i.imgur.com/C6xuBRM.png"
          alt="Residual Maps"
          style="width:80%;"
        />
        <p>
          <figcaption
            style="text-align:center;font-size:16px;font-style: italic;"
          >
            Fig. Residual Maps. Residual (left) and Percentage Residual (right)
          </figcaption>
        </p>
      </figure>
      <hr />
      <h2>
        Analysis and Conclusion
      </h2>
      <p>
        After trying out 5 different algorithms for getting the weights, we
        found out linear regression and random forest algorithms to yield the
        highest accuracy. The linear regression algorithm is
        also comparatively easier to code and run.
      </p>
      Link to presentation:
      <a
        href="https://docs.google.com/presentation/d/1jrNY_7aGx6P2EifGpFF8qe2vTx4WjXpAC9wye5m5qgk/present?usp=sharing"
        target="_blank"
        >link</a
      >.
      <p></p>

      <iframe
        src="https://docs.google.com/presentation/d/e/2PACX-1vQanQg0viNv8O6RsOG_Iv0EXxQPrPakStrj4V3WfR4EB959NPgSzlFo6Oot_rEw8eLzuYAtGdQDAy-9/embed?start=false&loop=false&delayms=3000"
        frameborder="0"
        width="960"
        height="569"
        allowfullscreen="true"
        mozallowfullscreen="true"
        webkitallowfullscreen="true"
      ></iframe>
      <p>
        Link to drive folder containing our codes:
        <a
          href="https://drive.google.com/drive/folders/1uK6JYk5KxtcymCpQYgJtX9czW2-HDxD-?usp=sharing"
          target="_blank"
          >link</a
        >.
      </p>

      <hr />
      <h3>References</h3>
      <ol>
        <li id="ref-1">
          <cite>
            "Detection of WHIM in the Planck data using Stack First approach"
          </cite>
          <a href="https://arxiv.org/abs/2001.08668"
            >https://arxiv.org/abs/2001.08668</a
          >.
        </li>
        <li id="ref-2">
          <cite>
            "Internal Linear Combination" method for the separation of CMB from
            Galactic foregrounds in the harmonic domain.
          </cite>
          <a href="https://arxiv.org/abs/0811.4277"
            >https://arxiv.org/abs/0811.4277</a
          >.
        </li>
        <li id="ref-3">
          <cite>
            Towards Data Science.
          </cite>
          <a href="https://towardsdatascience.com/">towardsdatascience.com/</a>.
        </li>
      </ol>

      <hr />
      <h3>Acknowledgements</h3>
      <p>
        We would like to thank Mr. Baibhav Singari and Dr. Tuhin Ghosh for his
        help in the project selection and guidance.
      </p>
      <hr />
      If you come across any errors, or if you have any suggestions or questions
      regarding this project, kindly contact us at:
      <a
        href="mailto: akshay.priyadarshi@niser.ac.in, pratyushk.das@niser.ac.in"
        target="_blank"
        >akshay.priyadarshi@niser.ac.in, pratyushk.das@niser.ac.in</a
      >
      <p>
        Odd Semester 2020-21 <br />
        Last updated on 15-12-2020. <br />
        ~ Akshay Priyadarshi (1611012)<sup>a</sup> & Pratyush Kumar Das
        (1611082)<sup>a</sup> <br />
        <sup>a</sup> School of Physical Sciences, National Institute of Science
        Education & Research (NISER), Bhubaneswar.
      </p>
    </div>
  </body>
</html>

